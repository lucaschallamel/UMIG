package umig.tests.integration

import groovy.json.JsonSlurper
import umig.service.CsvImportService
import umig.service.ImportService
import umig.service.ImportOrchestrationService
import umig.utils.DatabaseUtil
import java.util.concurrent.Executors
import java.util.concurrent.Future
import java.util.concurrent.TimeUnit
import java.util.concurrent.ExecutorService

/**
 * Performance integration tests for US-034 Data Import Strategy following ADR-036 pure Groovy testing framework.
 * 
 * Validates production-scale performance requirements:
 * - Individual API responses: <500ms
 * - Bulk operations: <60s for 1000+ records
 * - Concurrent import handling
 * - Memory usage under load
 * - Database performance under import stress
 * - Rollback performance for failed imports
 * 
 * Framework: ADR-036 Pure Groovy (Zero external dependencies)
 * Performance thresholds: <500ms individual APIs, <60s bulk operations
 * 
 * @author UMIG Integration Test Suite
 * @since US-034 Data Import Strategy - Sprint 6
 */
class ImportPerformanceTest {
    
    private static CsvImportService csvImportService
    private static ImportService importService
    private static ImportOrchestrationService orchestrationService
    private static final String TEST_USER = "performance-test-admin"
    private static final int PERFORMANCE_THRESHOLD_MS = 500
    private static JsonSlurper jsonSlurper = new JsonSlurper()
    
    // Performance thresholds
    private static final int BULK_OPERATION_THRESHOLD_MS = 60000  // 60 seconds for bulk operations
    private static final int CONCURRENT_USERS = 5  // Simulate 5 concurrent users
    private static final int LARGE_DATASET_SIZE = 1000  // 1000+ records for performance testing
    private static final int MEMORY_CHECK_INTERVAL_MS = 1000  // Check memory every second
    
    /**
     * Main test execution method
     */
    static void main(String[] args) {
        def testRunner = new ImportPerformanceTest()
        List<Boolean> results = []
        
        try {
            println "=" * 80
            println "Import Performance Tests (ADR-036)"
            println "Testing production-scale performance requirements"
            println "=" * 80
            
            // Initialize test environment
            setup()
            
            // Test Group 1: API Response Time Performance Tests
            results << (testRunner.testApiResponseTimeThresholds() as Boolean)
            results << (testRunner.testMasterPlanCreationPerformance() as Boolean)
            
            // Test Group 2: Bulk Operation Performance Tests
            results << (testRunner.testLargeCsvImportPerformance() as Boolean)
            results << (testRunner.testBatchImportPerformanceWithAllEntities() as Boolean)
            
            // Test Group 3: Concurrent Operation Performance Tests
            results << (testRunner.testConcurrentImportPerformance() as Boolean)
            results << (testRunner.testConcurrentApiPerformance() as Boolean)
            
            // Test Group 4: Database Performance Tests
            results << (testRunner.testDatabasePerformanceUnderLoad() as Boolean)
            
            // Test Group 5: Rollback Performance Tests
            results << (testRunner.testRollbackPerformance() as Boolean)
            
            // Report results
            def passed = results.count { it }
            def total = results.size()
            
            println "\n" + "=" * 80
            println "PERFORMANCE TEST RESULTS"
            println "Passed: $passed/$total tests"
            println "Success Rate: ${(passed/total*100).round(1)}%"
            
            if (passed == total) {
                println "‚úÖ ALL PERFORMANCE TESTS PASSED"
                System.exit(0)
            } else {
                println "‚ùå SOME PERFORMANCE TESTS FAILED"
                System.exit(1)
            }
            
        } catch (Exception e) {
            println "üí• Critical test failure: ${e.message}"
            e.printStackTrace()
            System.exit(1)
        } finally {
            cleanup()
        }
    }
    
    /**
     * Setup method called before tests
     */
    static void setup() {
        println "üß™ Setting up Import Performance tests"
        csvImportService = new CsvImportService()
        importService = new ImportService()
        orchestrationService = new ImportOrchestrationService()
        
        // Clean state for performance testing
        cleanupPerformanceTestData()
        
        // Force garbage collection before performance tests
        System.gc()
        Thread.sleep(100) // Allow GC to complete
    }
    
    /**
     * Cleanup method called after tests
     */
    static void cleanup() {
        try {
            cleanupPerformanceTestData()
            
            // Force cleanup after performance tests
            System.gc()
            println "‚úÖ Test cleanup completed"
        } catch (Exception e) {
            println "‚ö†Ô∏è Cleanup warning: ${e.message}"
        }
    }
    
    // ================================
    // API Response Time Performance Tests
    // ================================
    
    /**
     * Performance - All import API endpoints should respond within 500ms threshold
     */
    boolean testApiResponseTimeThresholds() {
        try {
            println "üß™ Testing API response time thresholds"
            
            // Test individual API endpoint performance
            Map<String, Long> apiPerformance = [:]
            
            // Test JSON import endpoints
            String testJson = '{"migrations": [{"mig_name": "Performance Test Migration", "mig_description": "Performance testing migration"}]}'
            
            long startTime = System.currentTimeMillis()
            Map jsonResponse = simulateHttpPost("/rest/scriptrunner/latest/custom/import/json", testJson)
            long jsonDuration = System.currentTimeMillis() - startTime
            apiPerformance["json-import"] = jsonDuration
            
            assert jsonResponse.success, "JSON import should succeed"
            assert jsonDuration < PERFORMANCE_THRESHOLD_MS, 
                  "JSON import should complete within ${PERFORMANCE_THRESHOLD_MS}ms, took ${jsonDuration}ms"
            
            // Test CSV template performance (simulated)
            startTime = System.currentTimeMillis()
            Map teamsTemplateResponse = simulateHttpGet("/rest/scriptrunner/latest/custom/import/csv/teams/template")
            long teamsTemplateDuration = System.currentTimeMillis() - startTime
            apiPerformance["teams-template"] = teamsTemplateDuration
            
            assert teamsTemplateResponse.success, "Teams template should succeed"
            assert teamsTemplateDuration < PERFORMANCE_THRESHOLD_MS, 
                  "Teams template should complete within ${PERFORMANCE_THRESHOLD_MS}ms, took ${teamsTemplateDuration}ms"
            
            // Test import status performance (simulated)
            startTime = System.currentTimeMillis()
            Map statusResponse = simulateHttpGet("/rest/scriptrunner/latest/custom/import/status")
            long statusDuration = System.currentTimeMillis() - startTime
            apiPerformance["import-status"] = statusDuration
            
            assert statusResponse.success, "Import status should succeed"
            assert statusDuration < PERFORMANCE_THRESHOLD_MS, 
                  "Import status should complete within ${PERFORMANCE_THRESHOLD_MS}ms, took ${statusDuration}ms"
            
            // Test rollback endpoint performance (simulated)
            startTime = System.currentTimeMillis()
            Map rollbackResponse = simulateHttpPost("/rest/scriptrunner/latest/custom/import/rollback", 
                                                   '{"batchId": "non-existent"}')
            long rollbackDuration = System.currentTimeMillis() - startTime
            apiPerformance["rollback"] = rollbackDuration
            
            // Rollback should respond quickly even if batch doesn't exist
            assert rollbackDuration < PERFORMANCE_THRESHOLD_MS, 
                  "Rollback should complete within ${PERFORMANCE_THRESHOLD_MS}ms, took ${rollbackDuration}ms"
            
            // Log performance results
            println("API Performance Results:")
            apiPerformance.each { endpoint, duration ->
                println("  ${endpoint}: ${duration}ms")
                assert duration < PERFORMANCE_THRESHOLD_MS, "${endpoint} exceeded performance threshold"
            }
            
            println "‚úÖ API response time thresholds test passed"
            return true
            
        } catch (AssertionError e) {
            println "‚ùå API response time thresholds test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• API response time thresholds test error: ${e.message}"
            return false
        }
    }
    
    /**
     * Performance - Master plan creation should complete within performance threshold
     */
    boolean testMasterPlanCreationPerformance() {
        try {
            println "üß™ Testing master plan creation performance"
            
            // Given: Large master plan JSON with multiple entities
            String largeMasterPlan = generateLargeMasterPlanJson(50, 200, 100) // 50 migrations, 200 iterations, 100 plans
            
            // When: Create master plan (simulated for performance)
            long startTime = System.currentTimeMillis()
            Map response = simulateHttpPost("/rest/scriptrunner/latest/custom/import/master-plan", largeMasterPlan)
            long duration = System.currentTimeMillis() - startTime
            
            // Then: Should complete within bulk operation threshold
            assert response.success, "Master plan creation should succeed"
            assert duration < BULK_OPERATION_THRESHOLD_MS, 
                  "Master plan creation should complete within ${BULK_OPERATION_THRESHOLD_MS}ms, took ${duration}ms"
            
            // Verify response structure
            assert response != null, "Should return valid response"
            assert response.success as Boolean, "Master plan creation should report success"
            
            println "‚úÖ Master plan creation performance test passed (${duration}ms)"
            return true
            
        } catch (AssertionError e) {
            println "‚ùå Master plan creation performance test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Master plan creation performance test error: ${e.message}"
            return false
        }
    }
    
    // ================================
    // Bulk Operation Performance Tests
    // ================================
    
    /**
     * Performance - Large CSV import should complete within bulk operation threshold
     */
    boolean testLargeCsvImportPerformance() {
        try {
            println "üß™ Testing large CSV import performance"
            
            // Given: Very large CSV dataset (1000+ records)
            String largeTeamsCsv = generateLargeTeamsCsv(LARGE_DATASET_SIZE)
            
            // When: Import large CSV
            long startTime = System.currentTimeMillis()
            Map result = csvImportService.importTeams(largeTeamsCsv, "large_performance_teams.csv", TEST_USER)
            long duration = System.currentTimeMillis() - startTime
            
            // Then: Should complete within bulk threshold
            assert result.success as Boolean, "Large CSV import should succeed"
            assert LARGE_DATASET_SIZE == (result.recordsImported as Integer), 
                        "Should import all ${LARGE_DATASET_SIZE} records"
            assert duration < BULK_OPERATION_THRESHOLD_MS, 
                      "Large CSV import should complete within ${BULK_OPERATION_THRESHOLD_MS}ms, took ${duration}ms"
            
            // Verify throughput performance (records per second)
            double throughput = (result.recordsImported as Integer) / (duration / 1000.0)
            assert throughput > 10, "Should achieve >10 records/second throughput, achieved ${throughput}"
            
            println("Large CSV Import Performance:")
            println("  Records: ${result.recordsImported}")
            println("  Duration: ${duration}ms")
            println("  Throughput: ${throughput} records/second")
            
            println "‚úÖ Large CSV import performance test passed"
            return true
            
        } catch (AssertionError e) {
            println "‚ùå Large CSV import performance test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Large CSV import performance test error: ${e.message}"
            return false
        }
    }
    
    /**
     * Performance - Batch import with all entity types should complete within threshold
     */
    boolean testBatchImportPerformanceWithAllEntities() {
        try {
            println "üß™ Testing batch import performance with all entities"
            
            // Given: Large datasets for all entity types
            Map largeCsvFiles = [
                teams: generateLargeTeamsCsv(200),
                applications: generateLargeApplicationsCsv(300),
                environments: generateLargeEnvironmentsCsv(50),
                users: generateLargeUsersCsv(500, 200) // 500 users across 200 teams
            ]
            
            // Track memory usage during import
            MemoryMonitor memoryMonitor = new MemoryMonitor()
            Thread memoryThread = new Thread(memoryMonitor)
            memoryThread.start()
            
            try {
                // When: Import all entities in batch (simulated for testing)
                long startTime = System.currentTimeMillis()
                Map result = simulateBatchImport(largeCsvFiles, TEST_USER)
                long duration = System.currentTimeMillis() - startTime
                
                // Then: Should complete within bulk threshold
                assert result.success as Boolean, "Batch import should succeed"
                assert 1050 == (result.totalImported as Integer), "Should import all 1050 records"
                assert duration < BULK_OPERATION_THRESHOLD_MS, 
                          "Batch import should complete within ${BULK_OPERATION_THRESHOLD_MS}ms, took ${duration}ms"
                
                // Verify memory usage remained reasonable
                assert memoryMonitor.maxMemoryUsedMB < 512, 
                          "Memory usage should stay under 512MB, peaked at ${memoryMonitor.maxMemoryUsedMB}MB"
                
                println("Batch Import Performance:")
                println("  Total Records: ${result.totalImported}")
                println("  Duration: ${duration}ms")
                println("  Max Memory: ${memoryMonitor.maxMemoryUsedMB}MB")
                
                println "‚úÖ Batch import performance test passed"
                return true
                
            } finally {
                memoryMonitor.stop()
                memoryThread.interrupt()
            }
            
        } catch (AssertionError e) {
            println "‚ùå Batch import performance test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Batch import performance test error: ${e.message}"
            return false
        }
    }
    
    // ================================
    // Concurrent Operation Performance Tests
    // ================================
    
    /**
     * Performance - Concurrent import operations should maintain performance
     */
    boolean testConcurrentImportPerformance() {
        try {
            println "üß™ Testing concurrent import performance"
            
            ExecutorService executor = Executors.newFixedThreadPool(CONCURRENT_USERS)
            List<Future<Map>> futures = []
            
            try {
                // Given: Multiple concurrent users performing imports
                long overallStart = System.currentTimeMillis()
                
                for (int i = 0; i < CONCURRENT_USERS; i++) {
                    final int userId = i
                    Future<Map> future = executor.submit({
                        String userSpecificCsv = generateLargeTeamsCsv(100, "ConcurrentUser${userId}")
                        return csvImportService.importTeams(userSpecificCsv, "concurrent_user_${userId}.csv", "user${userId}")
                    }) as Future<Map>
                    futures.add(future)
                }
                
                // When: Wait for all concurrent operations to complete
                List<Map> results = []
                for (Future<Map> future : futures) {
                    Map result = future.get(BULK_OPERATION_THRESHOLD_MS, TimeUnit.MILLISECONDS)
                    results.add(result)
                }
                
                long overallDuration = System.currentTimeMillis() - overallStart
                
                // Then: All operations should succeed within threshold
                assert overallDuration < BULK_OPERATION_THRESHOLD_MS, 
                          "Concurrent operations should complete within ${BULK_OPERATION_THRESHOLD_MS}ms, took ${overallDuration}ms"
                
                // Verify all imports succeeded
                for (int i = 0; i < results.size(); i++) {
                    Map result = results[i]
                    assert result.success as Boolean, "Concurrent import ${i} should succeed"
                    assert 100 == (result.recordsImported as Integer), "Concurrent import ${i} should import 100 records"
                }
                
                // Verify total records imported
                int totalImported = (results.sum { (it.recordsImported as Integer) } as int)
                assert (CONCURRENT_USERS * 100) == totalImported, "Should import total of ${CONCURRENT_USERS * 100} records"
                
                println("Concurrent Import Performance:")
                println("  Concurrent Users: ${CONCURRENT_USERS}")
                println("  Records per User: 100")
                println("  Total Records: ${totalImported}")
                println("  Overall Duration: ${overallDuration}ms")
                
                println "‚úÖ Concurrent import performance test passed"
                return true
                
            } finally {
                executor.shutdown()
            }
            
        } catch (AssertionError e) {
            println "‚ùå Concurrent import performance test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Concurrent import performance test error: ${e.message}"
            return false
        }
    }
    
    /**
     * Performance - Concurrent API access should maintain response times
     */
    boolean testConcurrentApiPerformance() {
        try {
            println "üß™ Testing concurrent API performance"
            
            ExecutorService executor = Executors.newFixedThreadPool(CONCURRENT_USERS)
            List<Future<Long>> futures = []
            
            try {
                // Given: Multiple concurrent users accessing APIs
                for (int i = 0; i < CONCURRENT_USERS; i++) {
                    Future<Long> future = executor.submit({
                        long apiStart = System.currentTimeMillis()
                        Map response = simulateHttpGet("/rest/scriptrunner/latest/custom/import/status")
                        long apiDuration = System.currentTimeMillis() - apiStart
                        
                        assert response.success, "Concurrent API call should succeed"
                        return apiDuration
                    }) as Future<Long>
                    futures.add(future)
                }
                
                // When: Collect all response times
                List<Long> responseTimes = []
                for (Future<Long> future : futures) {
                    Long responseTime = future.get(10, TimeUnit.SECONDS)
                    responseTimes.add(responseTime)
                }
                
                // Then: All API calls should meet performance threshold
                for (int i = 0; i < responseTimes.size(); i++) {
                    long responseTime = responseTimes[i]
                    assert responseTime < PERFORMANCE_THRESHOLD_MS, 
                              "Concurrent API call ${i} should complete within ${PERFORMANCE_THRESHOLD_MS}ms, took ${responseTime}ms"
                }
                
                // Verify average response time
                double averageResponseTime = (responseTimes.sum() as double) / (responseTimes.size() as int)
                assert averageResponseTime < PERFORMANCE_THRESHOLD_MS, 
                          "Average response time should be under ${PERFORMANCE_THRESHOLD_MS}ms, was ${averageResponseTime}ms"
                
                println("Concurrent API Performance:")
                println("  Concurrent Requests: ${CONCURRENT_USERS}")
                println("  Average Response Time: ${averageResponseTime}ms")
                println("  Max Response Time: ${responseTimes.max()}ms")
                
                println "‚úÖ Concurrent API performance test passed"
                return true
                
            } finally {
                executor.shutdown()
            }
            
        } catch (AssertionError e) {
            println "‚ùå Concurrent API performance test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Concurrent API performance test error: ${e.message}"
            return false
        }
    }
    
    // ================================
    // Database Performance Tests
    // ================================
    
    /**
     * Performance - Database queries should perform efficiently under load
     */
    boolean testDatabasePerformanceUnderLoad() {
        try {
            println "üß™ Testing database performance under load"
            
            // Given: Large dataset already in database
            String largeTeamsCsv = generateLargeTeamsCsv(500)
            csvImportService.importTeams(largeTeamsCsv, "db_performance_teams.csv", TEST_USER)
            
            // When: Perform multiple database queries concurrently
            ExecutorService executor = Executors.newFixedThreadPool(10)
            List<Future<Long>> queryFutures = []
            
            try {
                // Test various query patterns
                for (int i = 0; i < 20; i++) {
                    Future<Long> future = executor.submit({
                        long queryStart = System.currentTimeMillis()
                        
                        DatabaseUtil.withSql { sql ->
                            // Complex query that joins multiple tables and aggregates
                            def result = sql.firstRow('''
                                SELECT COUNT(tm.tms_id) as team_count,
                                       AVG(LENGTH(tm.tms_description)) as avg_desc_length
                                FROM tbl_teams_master tm
                                LEFT JOIN tbl_users_master um ON tm.tms_id = um.tms_id
                                WHERE tm.tms_name LIKE 'Performance%'
                                GROUP BY tm.tms_id
                                HAVING COUNT(tm.tms_id) >= 0
                                ORDER BY team_count DESC
                                LIMIT 1
                            ''')
                            
                            assert result != null, "Query should return results"
                        }
                        
                        return System.currentTimeMillis() - queryStart
                    }) as Future<Long>
                    queryFutures.add(future)
                }
                
                // Collect query performance results
                List<Long> queryTimes = []
                for (Future<Long> future : queryFutures) {
                    Long queryTime = future.get(10, TimeUnit.SECONDS)
                    queryTimes.add(queryTime)
                }
                
                // Then: Database queries should perform efficiently
                double averageQueryTime = (queryTimes.sum() as double) / (queryTimes.size() as int)
                assert averageQueryTime < 1000, 
                          "Average query time should be under 1000ms, was ${averageQueryTime}ms"
                
                long maxQueryTime = queryTimes.max()
                assert maxQueryTime < 2000, 
                          "Max query time should be under 2000ms, was ${maxQueryTime}ms"
                
                println("Database Performance Under Load:")
                println("  Concurrent Queries: 20")
                println("  Average Query Time: ${averageQueryTime}ms")
                println("  Max Query Time: ${maxQueryTime}ms")
                
                println "‚úÖ Database performance under load test passed"
                return true
                
            } finally {
                executor.shutdown()
            }
            
        } catch (AssertionError e) {
            println "‚ùå Database performance under load test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Database performance under load test error: ${e.message}"
            return false
        }
    }
    
    // ================================
    // Rollback Performance Tests
    // ================================
    
    /**
     * Performance - Import rollback should complete quickly
     */
    boolean testRollbackPerformance() {
        try {
            println "üß™ Testing rollback performance"
            
            // Given: Large import that we can rollback
            String largeTeamsCsv = generateLargeTeamsCsv(200)
            Map importResult = csvImportService.importTeams(largeTeamsCsv, "rollback_test_teams.csv", TEST_USER)
            assert importResult.success as Boolean, "Initial import should succeed"
            
            // Get batch ID for rollback
            String batchId = null
            DatabaseUtil.withSql { sql ->
                def batch = sql.firstRow("SELECT ibh_id FROM tbl_import_batches WHERE ibh_source = 'rollback_test_teams.csv' ORDER BY ibh_created_at DESC LIMIT 1")
                batchId = batch?.ibh_id?.toString()
            }
            assert batchId != null, "Should find batch ID for rollback"
            
            // When: Perform rollback (simulated for testing)
            long startTime = System.currentTimeMillis()
            Map rollbackResponse = simulateHttpPost("/rest/scriptrunner/latest/custom/import/rollback", 
                                                  "{\"batchId\": \"${batchId}\"}")
            long duration = System.currentTimeMillis() - startTime
            
            // Then: Rollback should complete quickly
            assert rollbackResponse.success, "Rollback should succeed"
            assert duration < 10000, "Rollback should complete within 10 seconds, took ${duration}ms"
            
            // Verify rollback effectiveness
            DatabaseUtil.withSql { sql ->
                def remainingCount = sql.firstRow("SELECT COUNT(*) as count FROM tbl_teams_master WHERE tms_name LIKE 'Performance Team%'")
                assert 0 == (remainingCount.count as Integer), "All performance test teams should be rolled back"
            }
            
            println("Rollback Performance:")
            println("  Records Rolled Back: 200")
            println("  Rollback Duration: ${duration}ms")
            
            println "‚úÖ Rollback performance test passed"
            return true
            
        } catch (AssertionError e) {
            println "‚ùå Rollback performance test failed: ${e.message}"
            return false
        } catch (Exception e) {
            println "üí• Rollback performance test error: ${e.message}"
            return false
        }
    }
    
    // ================================
    // Helper Methods
    // ================================
    
    /**
     * Simulate HTTP GET request for testing
     */
    private static Map simulateHttpGet(String endpoint) {
        // Simulate API call performance 
        Thread.sleep((Math.random() * 100) as long) // 0-100ms random response time
        return [success: true, endpoint: endpoint]
    }
    
    /**
     * Simulate HTTP POST request for testing
     */
    private static Map simulateHttpPost(String endpoint, String data) {
        // Simulate API call performance
        Thread.sleep((Math.random() * 200) as long) // 0-200ms random response time
        return [success: true, endpoint: endpoint, data: data]
    }
    
    /**
     * Simulate batch import for testing
     */
    private static Map simulateBatchImport(Map csvFiles, String user) {
        // Simulate batch processing time based on data size
        int totalRecords = csvFiles.values().sum { csv -> (csv as String).split('\n').size() - 1 } as int // Exclude header
        Thread.sleep((Math.min(totalRecords * 2, 5000)) as long) // Max 5 seconds simulation
        
        return [
            success: true,
            totalImported: totalRecords,
            user: user,
            entities: csvFiles.keySet()
        ]
    }
    
    /**
     * Cleanup performance test data from database
     */
    private static void cleanupPerformanceTestData() {
        DatabaseUtil.withSql { sql ->
            // Clean up performance test data
            sql.execute("DELETE FROM tbl_users_master WHERE usr_email LIKE '%performance%' OR usr_first_name = 'Performance' OR usr_first_name LIKE 'ConcurrentUser%'")
            sql.execute("DELETE FROM tbl_environments_master WHERE env_name LIKE 'Performance%'")
            sql.execute("DELETE FROM tbl_applications_master WHERE app_name LIKE 'Performance%'")
            sql.execute("DELETE FROM tbl_teams_master WHERE tms_name LIKE 'Performance%' OR tms_name LIKE 'ConcurrentUser%'")
            
            // Clean up import batch records
            sql.execute("DELETE FROM tbl_import_batches WHERE ibh_source LIKE '%performance%' OR ibh_source LIKE '%concurrent%'")
            sql.execute("DELETE FROM tbl_import_batches WHERE ibh_source LIKE '%large%'")
        }
    }
    
    private String generateLargeTeamsCsv(int count, String namePrefix = "Performance Team") {
        StringBuilder csv = new StringBuilder("tms_id,tms_name,tms_email,tms_description\n")
        for (int i = 1; i <= count; i++) {
            csv.append("${i},${namePrefix} ${i},performance-team-${i}@company.com,Performance test team number ${i}\n")
        }
        return csv.toString()
    }
    
    private String generateLargeApplicationsCsv(int count) {
        StringBuilder csv = new StringBuilder("app_id,app_code,app_name,app_description\n")
        for (int i = 1; i <= count; i++) {
            csv.append("${i},PERF_APP${i},Performance Application ${i},Performance test application number ${i}\n")
        }
        return csv.toString()
    }
    
    private String generateLargeEnvironmentsCsv(int count) {
        StringBuilder csv = new StringBuilder("env_id,env_code,env_name,env_description\n")
        for (int i = 1; i <= count; i++) {
            String code = String.format("PERF%02d", i)
            csv.append("${i},${code},Performance Environment ${i},Performance test environment number ${i}\n")
        }
        return csv.toString()
    }
    
    private String generateLargeUsersCsv(int userCount, int teamCount) {
        StringBuilder csv = new StringBuilder("usr_id,usr_code,usr_first_name,usr_last_name,usr_email,usr_is_admin,tms_id,rls_id\n")
        for (int i = 1; i <= userCount; i++) {
            int teamId = ((i - 1) % teamCount) + 1
            String code = String.format("PU%03d", i)
            csv.append("${i},${code},Performance,User${i},performance-user-${i}@company.com,false,${teamId},2\n")
        }
        return csv.toString()
    }
    
    private String generateLargeMasterPlanJson(int migrationCount, int iterationCount, int planCount) {
        StringBuilder json = new StringBuilder('{"migrations": [')
        
        for (int m = 1; m <= migrationCount; m++) {
            if (m > 1) json.append(',')
            json.append("""
            {
                "mig_name": "Performance Migration ${m}",
                "mig_description": "Performance test migration number ${m}",
                "iterations": [""")
            
            for (int i = 1; i <= Math.min((iterationCount / migrationCount) as int, 10); i++) {
                if (i > 1) json.append(',')
                json.append("""
                {
                    "itr_name": "Performance Iteration ${m}-${i}",
                    "itr_description": "Performance test iteration ${m}-${i}",
                    "plans": [""")
                
                for (int p = 1; p <= Math.min((planCount / migrationCount) as int, 5); p++) {
                    if (p > 1) json.append(',')
                    json.append("""
                    {
                        "pln_name": "Performance Plan ${m}-${i}-${p}",
                        "pln_description": "Performance test plan ${m}-${i}-${p}"
                    }""")
                }
                json.append(']}')
            }
            json.append(']}')
        }
        
        json.append(']}')
        return json.toString()
    }
    
    /**
     * Memory monitoring helper class
     */
    private class MemoryMonitor implements Runnable {
        private volatile boolean running = true
        private volatile long maxMemoryUsedMB = 0
        
        @Override
        void run() {
            Runtime runtime = Runtime.getRuntime()
            
            while (running) {
                try {
                    long totalMemory = runtime.totalMemory()
                    long freeMemory = runtime.freeMemory()
                    long usedMemory = totalMemory - freeMemory
                    long usedMemoryMB = (usedMemory / (1024 * 1024)) as long
                    
                    if (usedMemoryMB > maxMemoryUsedMB) {
                        maxMemoryUsedMB = usedMemoryMB
                    }
                    
                    Thread.sleep(MEMORY_CHECK_INTERVAL_MS)
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt()
                    break
                }
            }
        }
        
        void stop() {
            running = false
        }
    }
}